{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6625f464",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e585605",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88661001",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torchvision\n",
    "\n",
    "from torch import nn\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "638f4857",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68353f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup directory paths to train and test images\n",
    "train_dir = 'D:/tranformer_env/L-3_image_classification_using_ViT/custom_dataset/train'\n",
    "test_dir = 'D:/tranformer_env/L-3_image_classification_using_ViT/custom_dataset/test'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66a0626d",
   "metadata": {},
   "source": [
    "# Create Datasets and DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "828e061f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "NUM_WORKERS = os.cpu_count()\n",
    "\n",
    "def create_dataloaders(\n",
    "    train_dir: str, \n",
    "    test_dir: str, \n",
    "    transform: transforms.Compose, \n",
    "    batch_size: int, \n",
    "    num_workers: int=NUM_WORKERS\n",
    "):\n",
    "\n",
    "  # Use ImageFolder to create dataset(s)\n",
    "  train_data = datasets.ImageFolder(train_dir, transform=transform)\n",
    "  test_data = datasets.ImageFolder(test_dir, transform=transform)\n",
    "\n",
    "  # Get class names\n",
    "  class_names = train_data.classes\n",
    "\n",
    "  # Turn images into data loaders\n",
    "  train_dataloader = DataLoader(\n",
    "      train_data,\n",
    "      batch_size=batch_size,\n",
    "      shuffle=True,\n",
    "      num_workers=num_workers,\n",
    "      pin_memory=True,\n",
    "  )\n",
    "  test_dataloader = DataLoader(\n",
    "      test_data,\n",
    "      batch_size=batch_size,\n",
    "      shuffle=False,\n",
    "      num_workers=num_workers,\n",
    "      pin_memory=True,\n",
    "  )\n",
    "\n",
    "  return train_dataloader, test_dataloader, class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19da77f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create image size\n",
    "IMG_SIZE = 224\n",
    "\n",
    "# Create transform pipeline manually\n",
    "manual_transforms = transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "])           \n",
    "print(f\"Manually created transforms: {manual_transforms}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "308f1c2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the batch size\n",
    "BATCH_SIZE = 32 \n",
    "\n",
    "# Create data loaders\n",
    "train_dataloader, test_dataloader, class_names = create_dataloaders(\n",
    "    train_dir=train_dir,\n",
    "    test_dir=test_dir,\n",
    "    transform=manual_transforms, \n",
    "    batch_size=BATCH_SIZE\n",
    ")\n",
    "\n",
    "train_dataloader, test_dataloader, class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cd042d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's visualize a image in order to know if data is loaded properly or not\n",
    "\n",
    "# Get a batch of images\n",
    "image_batch, label_batch = next(iter(train_dataloader))\n",
    "\n",
    "# Get a single image from the batch\n",
    "image, label = image_batch[0], label_batch[0]\n",
    "\n",
    "# View the batch shapes\n",
    "print(image.shape, label)\n",
    "\n",
    "# Plot image with matplotlib\n",
    "plt.imshow(image.permute(1, 2, 0)) # rearrange image dimensions to suit matplotlib [color_channels, height, width] -> [height, width, color_channels]\n",
    "plt.title(class_names[label])\n",
    "plt.axis(False);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc53e072",
   "metadata": {},
   "source": [
    "# Step 1 \n",
    "\n",
    "#### Tasks:\n",
    "\n",
    "1- turn an image into patches\n",
    "\n",
    "2- flatten the patch feature maps into a single dimension\n",
    "\n",
    "3- Convert the output into Desried output (flattened 2D patches): (196, 768) -> N×(P2⋅C)       #Current shape: (1, 768, 196)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15d44424",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Create a class which subclasses nn.Module\n",
    "class PatchEmbedding(nn.Module):\n",
    "    \"\"\"Turns a 2D input image into a 1D sequence learnable embedding vector.\n",
    "    \n",
    "    Args:\n",
    "        in_channels (int): Number of color channels for the input images. Defaults to 3.\n",
    "        patch_size (int): Size of patches to convert input image into. Defaults to 16.\n",
    "        embedding_dim (int): Size of embedding to turn image into. Defaults to 768.\n",
    "    \"\"\" \n",
    "    # 2. Initialize the class with appropriate variables\n",
    "    def __init__(self, \n",
    "                 in_channels:int=3,\n",
    "                 patch_size:int=16,\n",
    "                 embedding_dim:int=768):\n",
    "        super().__init__()\n",
    "        \n",
    "        # 3. Create a layer to turn an image into patches\n",
    "        self.patcher = nn.Conv2d(in_channels=in_channels,\n",
    "                                 out_channels=embedding_dim,\n",
    "                                 kernel_size=patch_size,\n",
    "                                 stride=patch_size,\n",
    "                                 padding=0)\n",
    "\n",
    "        # 4. Create a layer to flatten the patch feature maps into a single dimension\n",
    "        self.flatten = nn.Flatten(start_dim=2, # only flatten the feature map dimensions into a single vector\n",
    "                                  end_dim=3)\n",
    "\n",
    "    # 5. Define the forward method \n",
    "    def forward(self, x):\n",
    "        # Create assertion to check that inputs are the correct shape\n",
    "        image_resolution = x.shape[-1]\n",
    "        assert image_resolution % patch_size == 0, f\"Input image size must be divisble by patch size, image shape: {image_resolution}, patch size: {patch_size}\"\n",
    "        \n",
    "        # Perform the forward pass\n",
    "        x_patched = self.patcher(x)\n",
    "        x_flattened = self.flatten(x_patched) \n",
    "        \n",
    "        # 6. Make sure the output shape has the right order \n",
    "        return x_flattened.permute(0, 2, 1) # adjust so the embedding is on the final dimension [batch_size, P^2•C, N] -> [batch_size, N, P^2•C]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93f238d6",
   "metadata": {},
   "source": [
    "# PatchEmbedding layer ready"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d991f3d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's test it on single image\n",
    "patch_size =16\n",
    "\n",
    "# Set seeds\n",
    "def set_seeds(seed: int=42):\n",
    "    \"\"\"Sets random sets for torch operations.\n",
    "\n",
    "    Args:\n",
    "        seed (int, optional): Random seed to set. Defaults to 42.\n",
    "    \"\"\"\n",
    "    # Set the seed for general torch operations\n",
    "    torch.manual_seed(seed)\n",
    "    # Set the seed for CUDA torch operations (ones that happen on the GPU)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    \n",
    "    \n",
    "    \n",
    "set_seeds()\n",
    "\n",
    "# Create an instance of patch embedding layer\n",
    "patchify = PatchEmbedding(in_channels=3,\n",
    "                          patch_size=16,\n",
    "                          embedding_dim=768)\n",
    "\n",
    "# Pass a single image through\n",
    "print(f\"Input image shape: {image.unsqueeze(0).shape}\")\n",
    "patch_embedded_image = patchify(image.unsqueeze(0)) # add an extra batch dimension on the 0th index, otherwise will error\n",
    "print(f\"Output patch embedding shape: {patch_embedded_image.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad3bae9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the patch embedding and patch embedding shape\n",
    "\n",
    "\n",
    "print(patch_embedded_image) \n",
    "print(f\"Patch embedding shape: {patch_embedded_image.shape} -> [batch_size, number_of_patches, embedding_dimension]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "111bb99f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now add the the learnable class embedding and position embeddings\n",
    "# From start to positional encoding: All in 1 cell\n",
    "\n",
    "set_seeds()\n",
    "\n",
    "# 1. Set patch size\n",
    "patch_size = 16\n",
    "\n",
    "# 2. Print shape of original image tensor and get the image dimensions\n",
    "print(f\"Image tensor shape: {image.shape}\")\n",
    "height, width = image.shape[1], image.shape[2]\n",
    "\n",
    "# 3. Get image tensor and add batch dimension\n",
    "x = image.unsqueeze(0)\n",
    "print(f\"Input image with batch dimension shape: {x.shape}\")\n",
    "\n",
    "# 4. Create patch embedding layer\n",
    "patch_embedding_layer = PatchEmbedding(in_channels=3,\n",
    "                                       patch_size=patch_size,\n",
    "                                       embedding_dim=768)\n",
    "\n",
    "# 5. Pass image through patch embedding layer\n",
    "patch_embedding = patch_embedding_layer(x)\n",
    "print(f\"Patching embedding shape: {patch_embedding.shape}\")\n",
    "\n",
    "\n",
    "\n",
    "# 6. Create class token embedding\n",
    "batch_size = patch_embedding.shape[0]\n",
    "embedding_dimension = patch_embedding.shape[-1]\n",
    "class_token = nn.Parameter(torch.ones(batch_size, 1, embedding_dimension),\n",
    "                           requires_grad=True) # make sure it's learnable\n",
    "print(f\"Class token embedding shape: {class_token.shape}\")\n",
    "\n",
    "# 7. Prepend class token embedding to patch embedding\n",
    "patch_embedding_class_token = torch.cat((class_token, patch_embedding), dim=1)\n",
    "print(f\"Patch embedding with class token shape: {patch_embedding_class_token.shape}\")\n",
    "\n",
    "# 8. Create position embedding\n",
    "number_of_patches = int((height * width) / patch_size**2)\n",
    "position_embedding = nn.Parameter(torch.ones(1, number_of_patches+1, embedding_dimension),\n",
    "                                  requires_grad=True) # make sure it's learnable\n",
    "\n",
    "\n",
    "# 9. Add position embedding to patch embedding with class token\n",
    "patch_and_position_embedding = patch_embedding_class_token + position_embedding\n",
    "print(f\"Patch and position embedding shape: {patch_and_position_embedding.shape}\")\n",
    "#patch_and_position_embedding\n",
    "\n",
    "print(patch_embedding_class_token)  #1 is added in the beginning of each\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50ecaae3",
   "metadata": {},
   "source": [
    "Here we're only creating the class token embedding as torch.ones() for demonstration purposes, in reality, you'd likely create the class token embedding with torch.randn() (since machine learning is all about harnessing the power of controlled randomness, you generally start with a random number and improve it over time)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aeb4eca",
   "metadata": {},
   "source": [
    "# Lets create layers used in Transformer's encoder:\n",
    "\n",
    "\n",
    "#### Norm (LN or LayerNorm) - torch.nn.LayerNorm().\n",
    "\n",
    "Layer Normalization (torch.nn.LayerNorm() or Norm or LayerNorm or LN) normalizes an input over the last dimension.\n",
    "\n",
    "Layer Normalization helps improve training time and model generalization (ability to adapt to unseen data).\n",
    "\n",
    "\n",
    "#### We can implement the MSA layer in PyTorch with torch.nn.MultiheadAttention() with the parameters:\n",
    "\n",
    "Multi-Head Self Attention (MSA) - <b>torch.nn.MultiheadAttention()</b>\n",
    "\n",
    "    embed_dim - the embedding dimension D .\n",
    "\n",
    "    num_heads - how many attention heads to use (this is where the term \"multihead\" comes from)\n",
    "\n",
    "    dropout - whether or not to apply dropout to the attention layer \n",
    "\n",
    "    batch_first - does our batch dimension come first? (yes it does)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5effd23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Create a class that inherits from nn.Module\n",
    "class MultiheadSelfAttentionBlock(nn.Module):\n",
    "    \"\"\"Creates a multi-head self-attention block (\"MSA block\" for short).\n",
    "    \"\"\"\n",
    "    # 2. Initialize the class with hyperparameters from Table 1\n",
    "    def __init__(self,\n",
    "                 embedding_dim:int=768, # Hidden size D from Table 1 for ViT-Base\n",
    "                 num_heads:int=12, # Heads from Table 1 for ViT-Base\n",
    "                 attn_dropout:float=0): # doesn't look like the paper uses any dropout in MSABlocks\n",
    "        super().__init__()\n",
    "        \n",
    "        # 3. Create the Norm layer (LN)\n",
    "        self.layer_norm = nn.LayerNorm(normalized_shape=embedding_dim)\n",
    "        \n",
    "        # 4. Create the Multi-Head Attention (MSA) layer\n",
    "        self.multihead_attn = nn.MultiheadAttention(embed_dim=embedding_dim,\n",
    "                                                    num_heads=num_heads,\n",
    "                                                    dropout=attn_dropout,\n",
    "                                                    batch_first=True) # does our batch dimension come first?\n",
    "        \n",
    "    # 5. Create a forward() method to pass the data throguh the layers\n",
    "    def forward(self, x):\n",
    "        x = self.layer_norm(x)\n",
    "        attn_output, _ = self.multihead_attn(query=x, # query embeddings \n",
    "                                             key=x, # key embeddings\n",
    "                                             value=x, # value embeddings\n",
    "                                             need_weights=False) # do we need the weights or just the layer outputs?\n",
    "        return attn_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f95822a",
   "metadata": {},
   "source": [
    "# MLP Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b61bd1d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Create a class that inherits from nn.Module\n",
    "class MLPBlock(nn.Module):\n",
    "    \"\"\"Creates a layer normalized multilayer perceptron block (\"MLP block\" for short).\"\"\"\n",
    "    # 2. Initialize the class with hyperparameters from Table 1 and Table 3\n",
    "    def __init__(self,\n",
    "                 embedding_dim:int=768, # Hidden Size D from Table 1 for ViT-Base\n",
    "                 mlp_size:int=3072, # MLP size from Table 1 for ViT-Base\n",
    "                 dropout:float=0.1): # Dropout from Table 3 for ViT-Base\n",
    "        super().__init__()\n",
    "        \n",
    "        # 3. Create the Norm layer (LN)\n",
    "        self.layer_norm = nn.LayerNorm(normalized_shape=embedding_dim)\n",
    "        \n",
    "        # 4. Create the Multilayer perceptron (MLP) layer(s)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(in_features=embedding_dim,\n",
    "                      out_features=mlp_size),\n",
    "            nn.GELU(), # \"The MLP contains two layers with a GELU non-linearity (section 3.1).\"\n",
    "            nn.Dropout(p=dropout),\n",
    "            nn.Linear(in_features=mlp_size, # needs to take same in_features as out_features of layer above\n",
    "                      out_features=embedding_dim), # take back to embedding_dim\n",
    "            nn.Dropout(p=dropout) # \"Dropout, when used, is applied after every dense layer..\"\n",
    "        )\n",
    "    \n",
    "    # 5. Create a forward() method to pass the data throguh the layers\n",
    "    def forward(self, x):\n",
    "        x = self.layer_norm(x)\n",
    "        x = self.mlp(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4066b0e",
   "metadata": {},
   "source": [
    "# Creating a Transformer Encoder by combining our custom made layers\n",
    "\n",
    "In below cell we are creating transformer encoder ourself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2c1ded8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Create a class that inherits from nn.Module\n",
    "class TransformerEncoderBlock(nn.Module):\n",
    "    \"\"\"Creates a Transformer Encoder block.\"\"\"\n",
    "    # 2. Initialize the class with hyperparameters from Table 1 and Table 3\n",
    "    def __init__(self,\n",
    "                 embedding_dim:int=768, # Hidden size D from Table 1 for ViT-Base\n",
    "                 num_heads:int=12, # Heads from Table 1 for ViT-Base\n",
    "                 mlp_size:int=3072, # MLP size from Table 1 for ViT-Base\n",
    "                 mlp_dropout:float=0.1, # Amount of dropout for dense layers from Table 3 for ViT-Base\n",
    "                 attn_dropout:float=0): # Amount of dropout for attention layers\n",
    "        super().__init__()\n",
    "\n",
    "        # 3. Create MSA block (equation 2)\n",
    "        self.msa_block = MultiheadSelfAttentionBlock(embedding_dim=embedding_dim,\n",
    "                                                     num_heads=num_heads,\n",
    "                                                     attn_dropout=attn_dropout)\n",
    "        \n",
    "        # 4. Create MLP block (equation 3)\n",
    "        self.mlp_block =  MLPBlock(embedding_dim=embedding_dim,\n",
    "                                   mlp_size=mlp_size,\n",
    "                                   dropout=mlp_dropout)\n",
    "        \n",
    "    # 5. Create a forward() method  \n",
    "    def forward(self, x):\n",
    "        \n",
    "        # 6. Create residual connection for MSA block (add the input to the output)\n",
    "        x =  self.msa_block(x) + x \n",
    "        \n",
    "        # 7. Create residual connection for MLP block (add the input to the output)\n",
    "        x = self.mlp_block(x) + x \n",
    "        \n",
    "        return x\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b408ef42",
   "metadata": {},
   "source": [
    "#### Transformer Encoder block created!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "114d4c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "transformer_encoder_block = TransformerEncoderBlock()\n",
    "\n",
    "from torchinfo import summary\n",
    "# # Print an input and output summary of our Transformer Encoder (uncomment for full output)\n",
    "summary(model=transformer_encoder_block,\n",
    "        input_size=(1, 197, 768), # (batch_size, num_patches, embedding_dimension)\n",
    "        col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
    "        col_width=20,\n",
    "       row_settings=[\"var_names\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d076ace",
   "metadata": {},
   "source": [
    "# Let's build a vision transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93b02def",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Create a ViT class that inherits from nn.Module\n",
    "class ViT(nn.Module):\n",
    "    \"\"\"Creates a Vision Transformer architecture with ViT-Base hyperparameters by default.\"\"\"\n",
    "    # 2. Initialize the class with hyperparameters from Table 1 and Table 3\n",
    "    def __init__(self,\n",
    "                 img_size:int=224, # Training resolution from Table 3 in ViT paper\n",
    "                 in_channels:int=3, # Number of channels in input image\n",
    "                 patch_size:int=16, # Patch size\n",
    "                 num_transformer_layers:int=12, # Layers from Table 1 for ViT-Base\n",
    "                 embedding_dim:int=768, # Hidden size D from Table 1 for ViT-Base\n",
    "                 mlp_size:int=3072, # MLP size from Table 1 for ViT-Base\n",
    "                 num_heads:int=12, # Heads from Table 1 for ViT-Base\n",
    "                 attn_dropout:float=0, # Dropout for attention projection\n",
    "                 mlp_dropout:float=0.1, # Dropout for dense/MLP layers \n",
    "                 embedding_dropout:float=0.1, # Dropout for patch and position embeddings\n",
    "                 num_classes:int=1000): # Default for ImageNet but can customize this\n",
    "        super().__init__() # don't forget the super().__init__()!\n",
    "        \n",
    "        # 3. Make the image size is divisble by the patch size \n",
    "        assert img_size % patch_size == 0, f\"Image size must be divisible by patch size, image size: {img_size}, patch size: {patch_size}.\"\n",
    "        \n",
    "        # 4. Calculate number of patches (height * width/patch^2)\n",
    "        self.num_patches = (img_size * img_size) // patch_size**2\n",
    "                 \n",
    "        # 5. Create learnable class embedding (needs to go at front of sequence of patch embeddings)\n",
    "        self.class_embedding = nn.Parameter(data=torch.randn(1, 1, embedding_dim),\n",
    "                                            requires_grad=True)\n",
    "        \n",
    "        # 6. Create learnable position embedding\n",
    "        self.position_embedding = nn.Parameter(data=torch.randn(1, self.num_patches+1, embedding_dim),\n",
    "                                               requires_grad=True)\n",
    "                \n",
    "        # 7. Create embedding dropout value\n",
    "        self.embedding_dropout = nn.Dropout(p=embedding_dropout)\n",
    "        \n",
    "        # 8. Create patch embedding layer\n",
    "        self.patch_embedding = PatchEmbedding(in_channels=in_channels,\n",
    "                                              patch_size=patch_size,\n",
    "                                              embedding_dim=embedding_dim)\n",
    "        \n",
    "        # 9. Create Transformer Encoder blocks (we can stack Transformer Encoder blocks using nn.Sequential()) \n",
    "        # Note: The \"*\" means \"all\"\n",
    "        self.transformer_encoder = nn.Sequential(*[TransformerEncoderBlock(embedding_dim=embedding_dim,\n",
    "                                                                            num_heads=num_heads,\n",
    "                                                                            mlp_size=mlp_size,\n",
    "                                                                            mlp_dropout=mlp_dropout) for _ in range(num_transformer_layers)])\n",
    "       \n",
    "        # 10. Create classifier head\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.LayerNorm(normalized_shape=embedding_dim),\n",
    "            nn.Linear(in_features=embedding_dim, \n",
    "                      out_features=num_classes)\n",
    "        )\n",
    "    \n",
    "    # 11. Create a forward() method\n",
    "    def forward(self, x):\n",
    "        \n",
    "        # 12. Get batch size\n",
    "        batch_size = x.shape[0]\n",
    "        \n",
    "        # 13. Create class token embedding and expand it to match the batch size (equation 1)\n",
    "        class_token = self.class_embedding.expand(batch_size, -1, -1) # \"-1\" means to infer the dimension (try this line on its own)\n",
    "\n",
    "        # 14. Create patch embedding (equation 1)\n",
    "        x = self.patch_embedding(x)\n",
    "\n",
    "        # 15. Concat class embedding and patch embedding (equation 1)\n",
    "        x = torch.cat((class_token, x), dim=1)\n",
    "\n",
    "        # 16. Add position embedding to patch embedding (equation 1) \n",
    "        x = self.position_embedding + x\n",
    "\n",
    "        # 17. Run embedding dropout (Appendix B.1)\n",
    "        x = self.embedding_dropout(x)\n",
    "\n",
    "        # 18. Pass patch, position and class embedding through transformer encoder layers (equations 2 & 3)\n",
    "        x = self.transformer_encoder(x)\n",
    "\n",
    "        # 19. Put 0 index logit through classifier (equation 4)\n",
    "        x = self.classifier(x[:, 0]) # run on each sample in a batch at 0 index\n",
    "\n",
    "        return x       "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32280991",
   "metadata": {},
   "source": [
    "# Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a9288fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train our MOdel\n",
    "\n",
    "# Create an instance of ViT with the number of classes we're working with (pizza, steak, sushi)\n",
    "vit = ViT(num_classes=len(class_names))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b421e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "from going_modular.going_modular import engine\n",
    "\n",
    "# Setup the optimizer to optimize our ViT model parameters using hyperparameters from the ViT paper \n",
    "optimizer = torch.optim.Adam(params=vit.parameters(), \n",
    "                             lr=3e-3, # Base LR from Table 3 for ViT-* ImageNet-1k\n",
    "                             betas=(0.9, 0.999), # default values but also mentioned in ViT paper section 4.1 (Training & Fine-tuning)\n",
    "                             weight_decay=0.3) # from the ViT paper section 4.1 (Training & Fine-tuning) and Table 3 for ViT-* ImageNet-1k\n",
    "\n",
    "# Setup the loss function for multi-class classification\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Set the seeds\n",
    "set_seeds()\n",
    "\n",
    "# Train the model and save the training results to a dictionary\n",
    "results = engine.train(model=vit,\n",
    "                       train_dataloader=train_dataloader,\n",
    "                       test_dataloader=test_dataloader,\n",
    "                       optimizer=optimizer,\n",
    "                       loss_fn=loss_fn,\n",
    "                       epochs=10,\n",
    "                       device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9e1a0a8",
   "metadata": {},
   "source": [
    "Our ViT model has come to life!\n",
    "\n",
    "Results on our custom dataset don't look too good.\n",
    "\n",
    "Lets plot the accuracy and loss curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd44080e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To check out our ViT model's loss curves, we can use the plot_loss_curves function from helper_functions.py\n",
    "from helper_functions import plot_loss_curves\n",
    "\n",
    "# Plot our ViT model's loss curves\n",
    "plot_loss_curves(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f575c15",
   "metadata": {},
   "source": [
    "# Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a180aa54",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "# Import function to make predictions on images and plot them \n",
    "from going_modular.going_modular.predictions import pred_and_plot_image\n",
    "\n",
    "# Setup custom image path\n",
    "custom_image_path = \"test_img.jpg\"\n",
    "\n",
    "# Predict on custom image\n",
    "pred_and_plot_image(model=vit,\n",
    "                    image_path=custom_image_path,\n",
    "                    class_names=class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f5a1a53",
   "metadata": {},
   "source": [
    "# Next part - Create Image classifier using pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb9ce187",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
